import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from mmdet.models import HEADS, build_loss
from mmdet.models.dense_heads import DETRHead
from mmdet3d.core.bbox.coders import build_bbox_coder
from mmcv.runner import force_fp32
from mmcv.cnn import Linear, bias_init_with_prob
from mmdet.models.utils.transformer import inverse_sigmoid
from mmdet.core.bbox.transforms import bbox_xyxy_to_cxcywh, bbox_cxcywh_to_xyxy
from mmdet.core import multi_apply, reduce_mean, build_assigner
from mmcv.utils import TORCH_VERSION, digit_version

def is_same(images_data, good_images):
    return (torch.allclose(images_data.cpu(),  good_images.cpu(), rtol=1e-03, atol=1e-05))


def compare_two_dicts(dict1, dict2):
    for key in dict1:
        if key not in dict2:
            continue
        if isinstance(dict1[key], dict) and isinstance(dict2[key], dict):
            return compare_two_dicts(dict1[key], dict2[key])
        if isinstance(dict1[key], torch.Tensor) and isinstance(dict2[key], torch.Tensor):
            return print(f'{key} in debug: {is_same(dict1[key], dict2[key])}')
        else:
            continue


class CameraFeatures(nn.Module):
    def __init__(self, model):
        super(CameraFeatures, self).__init__()
        self.img_backbone = model.img_backbone
        self.img_neck = model.img_neck
        
    def forward(self, img):
        # no grid mask is needed for inference.
        B, N, C, H, W = img.size()
        img = img.reshape(B * N, C, H, W)
        # input: tensor, output: dict
        img_feats = self.img_backbone(img)
        
        if isinstance(img_feats, dict):
            img_feats = list(img_feats.values())
        
        #input:list, out:tuple
        img_feats = self.img_neck(img_feats)
        
        img_feats_reshaped = []
        for img_feat in img_feats:
            BN, C, H, W = img_feat.size()
            img_feats_reshaped.append(img_feat.view(B, int(BN / B), C, H, W))
        return torch.cat(img_feats_reshaped, dim=0)



class lidarScn(nn.Module):
    def __init__(self, model):
        super(lidarScn, self).__init__()
        self.lidar_backbone = model.lidar_modal_extractor["backbone"]
        
    def forward(self, feats, coords, batch_size, sizes):
        return self.lidar_backbone(feats, coords, batch_size, sizes)



class BEVPooling(nn.Module):
    def __init__(self, model):
        super(BEVPooling, self).__init__()
        self.transformer = model.transformer
        # with weights
        self.bev_h = model.bev_h
        self.bev_w = model.bev_w
        self.feat_down_sample_indice = self.transformer.feat_down_sample_indice
        self.debug = {}
        pass
        
    def forward(
            self,
            img_feats,
            lidar_feat,
            img_meta
        ):
        
        mlvl_feats = [img_feats]
        mlvl_feats[0] = mlvl_feats[0].float()
        
        img_metas=[img_meta]
        
        # 1.1 get bev feats --> lss_bev_encode (pv2bev) + fuser(with lidar)
        images = mlvl_feats[self.feat_down_sample_indice] # -1, get the last element with largest resolution
        encoder_outputdict = self.transformer.encoder(images, img_metas)
        
        bev_embed = encoder_outputdict["bev"]
        depth = encoder_outputdict["depth"]
        bs, c, _, _ = bev_embed.shape
        bev_embed = bev_embed.view(bs, c, -1).permute(0, 2, 1).contiguous()
        ret_dict = dict(bev=bev_embed, depth=depth)
        
        # (2)fusion
        bev_embed = ret_dict["bev"]
        depth = ret_dict["depth"]
        
        if lidar_feat is not None and self.transformer.fuser is not None:
            bs = mlvl_feats[0].size(0)
            bev_embed = bev_embed.view(bs, self.bev_h, self.bev_w, -1)
            bev_embed = bev_embed.permute(0, 3, 1, 2).contiguous()
            #
            lidar_feat = lidar_feat.permute(0, 1, 3, 2).contiguous()  # B C H W
            lidar_feat = nn.functional.interpolate(lidar_feat, size=(self.bev_h, self.bev_w), mode="bicubic", align_corners=False)
            fused_bev = self.transformer.fuser([bev_embed, lidar_feat])
            fused_bev = fused_bev.flatten(2).permute(0, 2, 1).contiguous()
            bev_embed = fused_bev
            
        return bev_embed, depth


class BEVDecoder(nn.Module):
    def __init__(self, model):
        super(BEVDecoder, self).__init__()
        self.model = model
        
        # with weights
        self.reg_branches=self.model.reg_branches if self.model.with_box_refine else None
        self.cls_branches=self.model.cls_branches if self.model.as_two_stage else None
        self.pts_query = self.model.pts_embedding.weight.unsqueeze(0)
        
        self.z_cfg = self.model.z_cfg
        self.aux_seg = self.model.aux_seg
        
        # hyper-parameters
        self.num_vec_one2one = self.model.num_vec_one2one
        self.num_pts_per_vec = self.model.num_pts_per_vec
        
        self.bev_h = self.model.bev_h
        self.bev_w = self.model.bev_w
        self.grid_length=(self.model.real_h / self.model.bev_h, self.model.real_w / self.model.bev_w)
        
        self.feat_down_sample_indice = self.model.transformer.feat_down_sample_indice
        
        self.debug_key = None
        
        self.debug = {}
        
        pass  
    
    def set_key(self, key):
        self.debug_key = key
        
    def forward(
            self,
            img_feats,
            bev_embed,
            depth,
            img_metas,
            reg_branches=None,
            cls_branches=None,
            prev_bev=None,
            **kwargs, # need 'img_metas'
        ):
        
        # print(**kwargs)
        mlvl_feats = [img_feats]
        mlvl_feats[0] = mlvl_feats[0].float()
        
        ret_dict = dict(bev=bev_embed, depth=depth)
        
        bev_embed = ret_dict["bev"]
        depth = ret_dict["depth"]
        
        bs = mlvl_feats[0].size(0)
        bev_pos = None
        
        num_vec = self.num_vec_one2one
        self_attn_mask = torch.zeros([num_vec, num_vec]).bool().to(mlvl_feats[0].device)
        self_attn_mask[self.num_vec_one2one :, 0 : self.num_vec_one2one] = True
        self_attn_mask[0 : self.num_vec_one2one, self.num_vec_one2one :] = True
        kwargs["self_attn_mask"] = self_attn_mask
        
        (mask_pred_list, cls_pred_list, mask_aware_query_feat, bev_embed, bev_embed_ms, depth, dn_information, kwargs) = \
        self.model.transformer.IMPNet_part(
            mlvl_feats,
            ouput_dic=ret_dict,
            bev_pos=bev_pos,
            prev_bev=prev_bev,
            **kwargs
        )
        
        # self.debug["mask_pred_list"] = mask_pred_list[0]
        # self.debug["cls_pred_list"] = cls_pred_list[0]
        # self.debug["mask_aware_query_feat"] = mask_aware_query_feat
        # self.debug["bev_embed_ms"] = bev_embed_ms[0]
        # self.debug["dn_information"] = dn_information[0]
        
        # MMPnet
        # 2 call self.PositionalQueryGenerator
        pts_query = self.pts_query
        mask_dict, known_masks, known_bid, map_known_indice = dn_information
        mask_aware_query_feat, mask_aware_query_pos = self.model.transformer.PositionalQueryGenerator(
            mask_aware_query_feat,
            pts_query,
            mask_pred_list[-1],
            known_masks=known_masks,
            known_bid=known_bid,
            map_known_indice=map_known_indice,
        )
        
        # self.debug["mask_aware_query_feat1"] = mask_aware_query_feat
        # self.debug["mask_aware_query_pos"] = mask_aware_query_pos
        
        # 3 call self.GeometricFeatureExtractor
        
        # self.debug["mask_pred_list_1"] = mask_pred_list[0]
        # self.debug["mask_aware_query_feat_1"] = mask_aware_query_feat
        # self.debug["mask_aware_query_pos_1"] = mask_aware_query_pos
        # self.debug["bev_embed_1"] = bev_embed
        
        query, key, value, mask_aware_query, mask_aware_query_norm = self.model.transformer.GeometricFeatureExtractor(
            mask_pred_list,
            mask_aware_query_feat,
            mask_aware_query_pos,
            bev_embed,
        )
        
        # self.debug.update(self.model.transformer.debug)
        # self.debug["query"] = query
        # self.debug["key"] = key
        # self.debug["value"] = value
        # self.debug["mask_aware_query"] = mask_aware_query
        # self.debug["mask_aware_query_norm"] = mask_aware_query_norm
        
        # 4 call self.MaskGuidedMapDecoder
        kwargs["num_vec"] = num_vec
        kwargs["num_pts_per_vec"] = self.num_pts_per_vec
        
        reg_branches = self.reg_branches
        cls_branches = self.cls_branches
        
        inter_states, inter_references_out, init_reference_out = self.model.transformer.MaskGuidedMapDecoder(
            query,
            key,
            value,
            bev_embed_ms,
            mask_aware_query,
            mask_aware_query_norm,
            reg_branches,
            cls_branches,
            img_metas=img_metas,
            **kwargs,
        )
        
        # self.debug["mask_aware_query1"] = mask_aware_query
        # self.debug["mask_aware_query_norm1"] = mask_aware_query_norm
        # self.debug["inter_states"] = inter_states
        # self.debug["inter_references_out"] = inter_references_out
        # self.debug["init_reference_out"] = init_reference_out
        

        outputs = bev_embed, mask_pred_list, cls_pred_list, depth, inter_states, init_reference_out, inter_references_out, mask_dict

        bev_embed, segm_mask_pred_list, segm_cls_scores_list, depth, hs, init_reference, inter_references, mask_dict = outputs
        hs = hs.permute(0, 2, 1, 3)
        dn_pad_size = mask_dict["pad_size"] if mask_dict is not None else 0

        outputs_segm_mask_pred_dn = []
        outputs_segm_cls_scores_dn = []

        outputs_segm_mask_pred_one2one = []
        outputs_segm_cls_scores_one2one = []

        outputs_segm_mask_pred_one2many = []
        outputs_segm_cls_scores_one2many = []

        for lvl in range(len(segm_mask_pred_list)):
            outputs_segm_mask_pred_dn.append(segm_mask_pred_list[lvl][:, :dn_pad_size])
            outputs_segm_cls_scores_dn.append(segm_cls_scores_list[lvl][:, :dn_pad_size])

            outputs_segm_mask_pred_one2one.append(segm_mask_pred_list[lvl][:, dn_pad_size : dn_pad_size + self.num_vec_one2one])
            outputs_segm_cls_scores_one2one.append(segm_cls_scores_list[lvl][:, dn_pad_size : dn_pad_size + self.num_vec_one2one])

            outputs_segm_mask_pred_one2many.append(segm_mask_pred_list[lvl][:, dn_pad_size + self.num_vec_one2one :])
            outputs_segm_cls_scores_one2many.append(segm_cls_scores_list[lvl][:, dn_pad_size + self.num_vec_one2one :])

        outputs_segm_mask_pred_dn = torch.stack(outputs_segm_mask_pred_dn)
        outputs_segm_cls_scores_dn = torch.stack(outputs_segm_cls_scores_dn)
        outputs_segm_mask_pred_one2one = torch.stack(outputs_segm_mask_pred_one2one)
        outputs_segm_cls_scores_one2one = torch.stack(outputs_segm_cls_scores_one2one)
        outputs_segm_mask_pred_one2many = torch.stack(outputs_segm_mask_pred_one2many)
        outputs_segm_cls_scores_one2many = torch.stack(outputs_segm_cls_scores_one2many)

        outputs_classes_dn = []
        outputs_coords_dn = []
        outputs_pts_coords_dn = []

        outputs_classes_one2one = []
        outputs_coords_one2one = []
        outputs_pts_coords_one2one = []

        outputs_classes_one2many = []
        outputs_coords_one2many = []
        outputs_pts_coords_one2many = []

        num_vec = hs.shape[2] // self.num_pts_per_vec
        for lvl in range(hs.shape[0]):
            if lvl == 0:
                # import pdb;pdb.set_trace()
                reference = init_reference[..., 0:2] if not self.z_cfg["gt_z_flag"] else init_reference[..., 0:3]
            else:
                reference = inter_references[lvl - 1][..., 0:2] if not self.z_cfg["gt_z_flag"] else inter_references[lvl - 1][..., 0:3]
            reference = inverse_sigmoid(reference)
            outputs_class = self.model.cls_branches[lvl](hs[lvl].view(bs, num_vec, self.num_pts_per_vec, -1).mean(2))
            tmp = self.model.reg_branches[lvl](hs[lvl])
            tmp = tmp[..., 0:2] if not self.z_cfg["gt_z_flag"] else tmp[..., 0:3]
            tmp += reference

            tmp = tmp.sigmoid()  # cx,cy,w,h

            outputs_coord, outputs_pts_coord = self.model.transform_box(tmp, num_vec=num_vec)

            outputs_classes_one2one.append(outputs_class[:, dn_pad_size : dn_pad_size + self.num_vec_one2one])
            outputs_coords_one2one.append(outputs_coord[:, dn_pad_size : dn_pad_size + self.num_vec_one2one])
            outputs_pts_coords_one2one.append(outputs_pts_coord[:, dn_pad_size : dn_pad_size + self.num_vec_one2one])

            outputs_classes_dn.append(outputs_class[:, :dn_pad_size])
            outputs_coords_dn.append(outputs_coord[:, :dn_pad_size])
            outputs_pts_coords_dn.append(outputs_pts_coord[:, :dn_pad_size])

            outputs_classes_one2many.append(outputs_class[:, dn_pad_size + self.num_vec_one2one :])
            outputs_coords_one2many.append(outputs_coord[:, dn_pad_size + self.num_vec_one2one :])
            outputs_pts_coords_one2many.append(outputs_pts_coord[:, dn_pad_size + self.num_vec_one2one :])

        outputs_classes_dn = torch.stack(outputs_classes_dn)
        outputs_coords_dn = torch.stack(outputs_coords_dn)
        outputs_pts_coords_dn = torch.stack(outputs_pts_coords_dn)

        outputs_classes_one2one = torch.stack(outputs_classes_one2one)
        outputs_coords_one2one = torch.stack(outputs_coords_one2one)
        outputs_pts_coords_one2one = torch.stack(outputs_pts_coords_one2one)

        outputs_classes_one2many = torch.stack(outputs_classes_one2many)
        outputs_coords_one2many = torch.stack(outputs_coords_one2many)
        outputs_pts_coords_one2many = torch.stack(outputs_pts_coords_one2many)

        if self.model.dn_enabled and self.model.training:
            mask_dict["output_known_lbs_bboxes"] = (
                outputs_segm_cls_scores_dn,
                outputs_segm_mask_pred_dn,
                outputs_classes_dn,
                outputs_coords_dn,
                outputs_pts_coords_dn,
            )

        outputs_seg = None
        outputs_pv_seg = None
        if self.aux_seg["use_aux_seg"]:
            seg_bev_embed = \
                bev_embed.permute(1, 0, 2).reshape(bs, self.model.bev_h, self.model.bev_w, -1).permute(0, 3, 1, 2).contiguous()
            if self.aux_seg["bev_seg"]:
                outputs_seg = self.model.seg_head(seg_bev_embed)
            bs, num_cam, embed_dims, feat_h, feat_w = mlvl_feats[-1].shape
            if self.aux_seg["pv_seg"]:
                outputs_pv_seg = self.model.pv_seg_head(mlvl_feats[-1].flatten(0, 1))
                outputs_pv_seg = outputs_pv_seg.view(bs, num_cam, -1, feat_h, feat_w)

        if self.model.tr_cls_join and self.model.training:
            outputs_classes_one2one = outputs_classes_one2one + outputs_segm_cls_scores_one2one[-1:][..., :3].contiguous()

        if self.model.cls_join and not self.model.training:
            outputs_classes_one2one = outputs_classes_one2one + outputs_segm_cls_scores_one2one[-1:][..., :3].contiguous()
        outs = {
            "bev_embed": bev_embed,
            "all_cls_scores": outputs_classes_one2one,
            "all_bbox_preds": outputs_coords_one2one,
            "all_pts_preds": outputs_pts_coords_one2one,
            "enc_bbox_preds": None,
            "enc_pts_preds": None,
            "depth": depth,
            "seg": outputs_seg,
            "segm_mask_preds": outputs_segm_mask_pred_one2one,
            "segm_cls_scores": outputs_segm_cls_scores_one2one,
            "pv_seg": outputs_pv_seg,
            "one2many_outs": dict(
                all_cls_scores=outputs_classes_one2many,
                all_bbox_preds=outputs_coords_one2many,
                all_pts_preds=outputs_pts_coords_one2many,
                enc_bbox_preds=None,
                enc_pts_preds=None,
                segm_mask_preds=outputs_segm_mask_pred_one2many,
                segm_cls_scores=outputs_segm_cls_scores_one2many,
                seg=None,
                pv_seg=None,
            ),
            "dn_mask_dict": mask_dict,
            "debug": self.debug,
        }

        return outs
        
        # return self.get_bboxes(outs, img_metas, rescale=False)

    @force_fp32(apply_to=("preds_dicts"))
    def get_bboxes(self, preds_dicts, img_metas, rescale=False):
        # bboxes: xmin, ymin, xmax, ymax
        preds_dicts = self.model.bbox_coder.decode(preds_dicts)

        num_samples = len(preds_dicts)
        ret_list = []
        for i in range(num_samples):
            preds = preds_dicts[i]
            bboxes = preds["bboxes"]
            # bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5

            # code_size = bboxes.shape[-1]
            # bboxes = img_metas[i]['box_type_3d'](bboxes, code_size)
            scores = preds["scores"]
            labels = preds["labels"]
            pts = preds["pts"]

            ret_list.append([bboxes, scores, labels, pts])

        return ret_list

class DetrHead(nn.Module):
    def __init__(self, model):
        super(DetrHead, self).__init__()
        self.model = model
        self.num_vec_one2one = self.model.num_vec_one2one
        self.aux_seg = self.model.aux_seg
        self.z_cfg = self.model.z_cfg
        self.num_pts_per_vec = self.model.num_pts_per_vec
    
    def get_debug_key(self):
        return self.model.transformer.debug['key']
    
    @force_fp32(apply_to=("mlvl_feats", "prev_bev"))
    def forward(self, mlvl_feats, lidar_feat, img_metas, prev_bev=None, only_bev=False):
        mlvl_feats[0] = mlvl_feats[0].float()
        num_vec = self.num_vec_one2one

        bs, num_cam, _, _, _ = mlvl_feats[0].shape
        dtype = mlvl_feats[0].dtype

        pts_query = self.model.pts_embedding.weight.unsqueeze(0)

        bev_queries = None
        bev_mask = None
        bev_pos = None

        # make attn mask
        """ attention mask to prevent information leakage
        """
        self_attn_mask = torch.zeros([num_vec, num_vec]).bool().to(mlvl_feats[0].device)
        self_attn_mask[self.num_vec_one2one :, 0 : self.num_vec_one2one] = True
        self_attn_mask[0 : self.num_vec_one2one, self.num_vec_one2one :] = True

        outputs = self.model.transformer(
            mlvl_feats,
            lidar_feat,
            bev_queries,
            pts_query,
            self.model.bev_h,
            self.model.bev_w,
            grid_length=(self.model.real_h / self.model.bev_h, self.model.real_w / self.model.bev_w),
            bev_pos=bev_pos,
            reg_branches=(self.model.reg_branches if self.model.with_box_refine else None),  # noqa:E501
            cls_branches=self.model.cls_branches if self.model.as_two_stage else None,
            img_metas=img_metas,
            prev_bev=prev_bev,
            self_attn_mask=self_attn_mask,
            num_vec=num_vec,
            num_pts_per_vec=self.num_pts_per_vec,
        )
        
        # bev_embed, segm_mask_pred_list, segm_cls_scores_list, depth, hs, init_reference, inter_references, mask_dict = outputs
        
        bev_embed, segm_mask_pred_list, segm_cls_scores_list, depth, hs, init_reference, inter_references, mask_dict = outputs
        
        hs = hs.permute(0, 2, 1, 3)
        dn_pad_size = mask_dict["pad_size"] if mask_dict is not None else 0

        outputs_segm_mask_pred_dn = []
        outputs_segm_cls_scores_dn = []

        outputs_segm_mask_pred_one2one = []
        outputs_segm_cls_scores_one2one = []

        outputs_segm_mask_pred_one2many = []
        outputs_segm_cls_scores_one2many = []

        for lvl in range(len(segm_mask_pred_list)):
            outputs_segm_mask_pred_dn.append(segm_mask_pred_list[lvl][:, :dn_pad_size])
            outputs_segm_cls_scores_dn.append(segm_cls_scores_list[lvl][:, :dn_pad_size])

            outputs_segm_mask_pred_one2one.append(segm_mask_pred_list[lvl][:, dn_pad_size : dn_pad_size + self.num_vec_one2one])
            outputs_segm_cls_scores_one2one.append(segm_cls_scores_list[lvl][:, dn_pad_size : dn_pad_size + self.num_vec_one2one])

            outputs_segm_mask_pred_one2many.append(segm_mask_pred_list[lvl][:, dn_pad_size + self.num_vec_one2one :])
            outputs_segm_cls_scores_one2many.append(segm_cls_scores_list[lvl][:, dn_pad_size + self.num_vec_one2one :])

        outputs_segm_mask_pred_dn = torch.stack(outputs_segm_mask_pred_dn)
        outputs_segm_cls_scores_dn = torch.stack(outputs_segm_cls_scores_dn)
        outputs_segm_mask_pred_one2one = torch.stack(outputs_segm_mask_pred_one2one)
        outputs_segm_cls_scores_one2one = torch.stack(outputs_segm_cls_scores_one2one)
        outputs_segm_mask_pred_one2many = torch.stack(outputs_segm_mask_pred_one2many)
        outputs_segm_cls_scores_one2many = torch.stack(outputs_segm_cls_scores_one2many)

        outputs_classes_dn = []
        outputs_coords_dn = []
        outputs_pts_coords_dn = []

        outputs_classes_one2one = []
        outputs_coords_one2one = []
        outputs_pts_coords_one2one = []

        outputs_classes_one2many = []
        outputs_coords_one2many = []
        outputs_pts_coords_one2many = []

        num_vec = hs.shape[2] // self.num_pts_per_vec
        for lvl in range(hs.shape[0]):
            if lvl == 0:
                # import pdb;pdb.set_trace()
                reference = init_reference[..., 0:2] if not self.z_cfg["gt_z_flag"] else init_reference[..., 0:3]
            else:
                reference = inter_references[lvl - 1][..., 0:2] if not self.z_cfg["gt_z_flag"] else inter_references[lvl - 1][..., 0:3]
            reference = inverse_sigmoid(reference)
            outputs_class = self.model.cls_branches[lvl](hs[lvl].view(bs, num_vec, self.num_pts_per_vec, -1).mean(2))
            tmp = self.model.reg_branches[lvl](hs[lvl])
            tmp = tmp[..., 0:2] if not self.z_cfg["gt_z_flag"] else tmp[..., 0:3]
            tmp += reference

            tmp = tmp.sigmoid()  # cx,cy,w,h

            outputs_coord, outputs_pts_coord = self.model.transform_box(tmp, num_vec=num_vec)

            outputs_classes_one2one.append(outputs_class[:, dn_pad_size : dn_pad_size + self.num_vec_one2one])
            outputs_coords_one2one.append(outputs_coord[:, dn_pad_size : dn_pad_size + self.num_vec_one2one])
            outputs_pts_coords_one2one.append(outputs_pts_coord[:, dn_pad_size : dn_pad_size + self.num_vec_one2one])

            outputs_classes_dn.append(outputs_class[:, :dn_pad_size])
            outputs_coords_dn.append(outputs_coord[:, :dn_pad_size])
            outputs_pts_coords_dn.append(outputs_pts_coord[:, :dn_pad_size])

            outputs_classes_one2many.append(outputs_class[:, dn_pad_size + self.num_vec_one2one :])
            outputs_coords_one2many.append(outputs_coord[:, dn_pad_size + self.num_vec_one2one :])
            outputs_pts_coords_one2many.append(outputs_pts_coord[:, dn_pad_size + self.num_vec_one2one :])

        outputs_classes_dn = torch.stack(outputs_classes_dn)
        outputs_coords_dn = torch.stack(outputs_coords_dn)
        outputs_pts_coords_dn = torch.stack(outputs_pts_coords_dn)

        outputs_classes_one2one = torch.stack(outputs_classes_one2one)
        outputs_coords_one2one = torch.stack(outputs_coords_one2one)
        outputs_pts_coords_one2one = torch.stack(outputs_pts_coords_one2one)

        outputs_classes_one2many = torch.stack(outputs_classes_one2many)
        outputs_coords_one2many = torch.stack(outputs_coords_one2many)
        outputs_pts_coords_one2many = torch.stack(outputs_pts_coords_one2many)

        if self.model.dn_enabled and self.model.training:
            mask_dict["output_known_lbs_bboxes"] = (
                outputs_segm_cls_scores_dn,
                outputs_segm_mask_pred_dn,
                outputs_classes_dn,
                outputs_coords_dn,
                outputs_pts_coords_dn,
            )

        outputs_seg = None
        outputs_pv_seg = None
        if self.aux_seg["use_aux_seg"]:
            seg_bev_embed = \
                bev_embed.permute(1, 0, 2).reshape(bs, self.model.bev_h, self.model.bev_w, -1).permute(0, 3, 1, 2).contiguous()
            if self.aux_seg["bev_seg"]:
                outputs_seg = self.model.seg_head(seg_bev_embed)
            bs, num_cam, embed_dims, feat_h, feat_w = mlvl_feats[-1].shape
            if self.aux_seg["pv_seg"]:
                outputs_pv_seg = self.model.pv_seg_head(mlvl_feats[-1].flatten(0, 1))
                outputs_pv_seg = outputs_pv_seg.view(bs, num_cam, -1, feat_h, feat_w)

        if self.model.tr_cls_join and self.model.training:
            outputs_classes_one2one = outputs_classes_one2one + outputs_segm_cls_scores_one2one[-1:][..., :3].contiguous()

        if self.model.cls_join and not self.model.training:
            outputs_classes_one2one = outputs_classes_one2one + outputs_segm_cls_scores_one2one[-1:][..., :3].contiguous()
        outs = {
            "bev_embed": bev_embed,
            "all_cls_scores": outputs_classes_one2one,
            "all_bbox_preds": outputs_coords_one2one,
            "all_pts_preds": outputs_pts_coords_one2one,
            "enc_bbox_preds": None,
            "enc_pts_preds": None,
            "depth": depth,
            "seg": outputs_seg,
            "segm_mask_preds": outputs_segm_mask_pred_one2one,
            "segm_cls_scores": outputs_segm_cls_scores_one2one,
            "pv_seg": outputs_pv_seg,
            "one2many_outs": dict(
                all_cls_scores=outputs_classes_one2many,
                all_bbox_preds=outputs_coords_one2many,
                all_pts_preds=outputs_pts_coords_one2many,
                enc_bbox_preds=None,
                enc_pts_preds=None,
                segm_mask_preds=outputs_segm_mask_pred_one2many,
                segm_cls_scores=outputs_segm_cls_scores_one2many,
                seg=None,
                pv_seg=None,
            ),
            "dn_mask_dict": mask_dict,
            "debug": self.model.transformer.debug,
        }

        return outs

        return self.get_bboxes(outs, img_metas, rescale=False)
    
    @force_fp32(apply_to=("preds_dicts"))
    def get_bboxes(self, preds_dicts, img_metas, rescale=False):
        # bboxes: xmin, ymin, xmax, ymax
        preds_dicts = self.model.bbox_coder.decode(preds_dicts)

        num_samples = len(preds_dicts)
        ret_list = []
        for i in range(num_samples):
            preds = preds_dicts[i]
            bboxes = preds["bboxes"]
            # bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5

            # code_size = bboxes.shape[-1]
            # bboxes = img_metas[i]['box_type_3d'](bboxes, code_size)
            scores = preds["scores"]
            labels = preds["labels"]
            pts = preds["pts"]

            ret_list.append([bboxes, scores, labels, pts])

        return ret_list